{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for Fraud Detection: Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "#from dnn_app_utils_v3 import *\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import keras\n",
    "print(tf.__version__)\n",
    "from tensorflow.python.framework import ops\n",
    "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The data for credit card fraud detection is from: \n",
    "https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"./creditcardfraud/creditcard.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(d, test_size=0.2)\n",
    "train_y = train.pop('Class')\n",
    "train_x_orig = train\n",
    "test_y = test.pop('Class')\n",
    "test_x_orig = test\n",
    "#train_x_orig = train[:,0:30] .values\n",
    "#train_y = train['Class'].values\n",
    "#test_x_orig = test[:,0:30]\n",
    "#test_y = test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 227845\n",
      "Number of testing examples: 56962\n",
      "train_x_orig shape: (227845, 30)\n",
      "train_y shape: (227845,)\n",
      "test_x_orig shape: (56962, 30)\n",
      "test_y shape: (56962,)\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset\n",
    "m_train = train_x_orig.shape[0]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13367</th>\n",
       "      <td>23610.0</td>\n",
       "      <td>1.387525</td>\n",
       "      <td>-0.547811</td>\n",
       "      <td>0.076422</td>\n",
       "      <td>-0.739334</td>\n",
       "      <td>-0.519887</td>\n",
       "      <td>-0.026007</td>\n",
       "      <td>-0.694673</td>\n",
       "      <td>-0.052100</td>\n",
       "      <td>0.378187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.469630</td>\n",
       "      <td>-0.658299</td>\n",
       "      <td>-1.307286</td>\n",
       "      <td>-0.013221</td>\n",
       "      <td>-0.912415</td>\n",
       "      <td>0.163178</td>\n",
       "      <td>0.920997</td>\n",
       "      <td>-0.096560</td>\n",
       "      <td>-0.014846</td>\n",
       "      <td>31.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31788</th>\n",
       "      <td>36516.0</td>\n",
       "      <td>1.021179</td>\n",
       "      <td>-0.194114</td>\n",
       "      <td>1.207791</td>\n",
       "      <td>1.738369</td>\n",
       "      <td>-0.659824</td>\n",
       "      <td>0.631864</td>\n",
       "      <td>-0.434168</td>\n",
       "      <td>0.227422</td>\n",
       "      <td>1.116400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090960</td>\n",
       "      <td>-0.386993</td>\n",
       "      <td>-0.664598</td>\n",
       "      <td>0.047173</td>\n",
       "      <td>0.106897</td>\n",
       "      <td>0.443932</td>\n",
       "      <td>-0.515898</td>\n",
       "      <td>0.091188</td>\n",
       "      <td>0.035959</td>\n",
       "      <td>41.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84796</th>\n",
       "      <td>60465.0</td>\n",
       "      <td>-1.134453</td>\n",
       "      <td>0.636843</td>\n",
       "      <td>2.457241</td>\n",
       "      <td>1.317848</td>\n",
       "      <td>0.605755</td>\n",
       "      <td>0.143546</td>\n",
       "      <td>0.242076</td>\n",
       "      <td>0.371466</td>\n",
       "      <td>-1.315582</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040412</td>\n",
       "      <td>0.328508</td>\n",
       "      <td>0.683118</td>\n",
       "      <td>-0.114294</td>\n",
       "      <td>0.203942</td>\n",
       "      <td>0.035426</td>\n",
       "      <td>-0.090987</td>\n",
       "      <td>0.057247</td>\n",
       "      <td>0.102961</td>\n",
       "      <td>23.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6243</th>\n",
       "      <td>7309.0</td>\n",
       "      <td>-0.851343</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.406917</td>\n",
       "      <td>-1.669375</td>\n",
       "      <td>0.462639</td>\n",
       "      <td>-0.150804</td>\n",
       "      <td>0.403887</td>\n",
       "      <td>0.200460</td>\n",
       "      <td>1.968672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.301203</td>\n",
       "      <td>-0.400262</td>\n",
       "      <td>-1.096108</td>\n",
       "      <td>0.023423</td>\n",
       "      <td>-0.842077</td>\n",
       "      <td>-0.301144</td>\n",
       "      <td>0.932506</td>\n",
       "      <td>-0.088128</td>\n",
       "      <td>0.076487</td>\n",
       "      <td>45.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162432</th>\n",
       "      <td>115111.0</td>\n",
       "      <td>-0.040314</td>\n",
       "      <td>0.545458</td>\n",
       "      <td>0.831788</td>\n",
       "      <td>-0.515723</td>\n",
       "      <td>0.128584</td>\n",
       "      <td>-0.112466</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.222503</td>\n",
       "      <td>0.639397</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120943</td>\n",
       "      <td>0.102439</td>\n",
       "      <td>0.351638</td>\n",
       "      <td>0.180840</td>\n",
       "      <td>0.572238</td>\n",
       "      <td>-1.486248</td>\n",
       "      <td>0.147366</td>\n",
       "      <td>0.253324</td>\n",
       "      <td>0.291266</td>\n",
       "      <td>9.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "13367    23610.0  1.387525 -0.547811  0.076422 -0.739334 -0.519887 -0.026007   \n",
       "31788    36516.0  1.021179 -0.194114  1.207791  1.738369 -0.659824  0.631864   \n",
       "84796    60465.0 -1.134453  0.636843  2.457241  1.317848  0.605755  0.143546   \n",
       "6243      7309.0 -0.851343  0.062000  1.406917 -1.669375  0.462639 -0.150804   \n",
       "162432  115111.0 -0.040314  0.545458  0.831788 -0.515723  0.128584 -0.112466   \n",
       "\n",
       "              V7        V8        V9   ...         V20       V21       V22  \\\n",
       "13367  -0.694673 -0.052100  0.378187   ...   -0.469630 -0.658299 -1.307286   \n",
       "31788  -0.434168  0.227422  1.116400   ...   -0.090960 -0.386993 -0.664598   \n",
       "84796   0.242076  0.371466 -1.315582   ...   -0.040412  0.328508  0.683118   \n",
       "6243    0.403887  0.200460  1.968672   ...   -0.301203 -0.400262 -1.096108   \n",
       "162432  0.027800  0.222503  0.639397   ...   -0.120943  0.102439  0.351638   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \n",
       "13367  -0.013221 -0.912415  0.163178  0.920997 -0.096560 -0.014846   31.35  \n",
       "31788   0.047173  0.106897  0.443932 -0.515898  0.091188  0.035959   41.29  \n",
       "84796  -0.114294  0.203942  0.035426 -0.090987  0.057247  0.102961   23.27  \n",
       "6243    0.023423 -0.842077 -0.301144  0.932506 -0.088128  0.076487   45.00  \n",
       "162432  0.180840  0.572238 -1.486248  0.147366  0.253324  0.291266    9.84  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore further\n",
    "train_x_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.36100000e+04,  1.38752494e+00, -5.47811116e-01, ...,\n",
       "        -9.65601597e-02, -1.48461215e-02,  3.13500000e+01],\n",
       "       [ 3.65160000e+04,  1.02117876e+00, -1.94113665e-01, ...,\n",
       "         9.11875105e-02,  3.59594694e-02,  4.12900000e+01],\n",
       "       [ 6.04650000e+04, -1.13445277e+00,  6.36842930e-01, ...,\n",
       "         5.72469651e-02,  1.02960891e-01,  2.32700000e+01],\n",
       "       ...,\n",
       "       [ 5.50950000e+04, -1.14996324e+00,  1.69646167e+00, ...,\n",
       "        -1.62046536e-01,  3.11927890e-02,  7.58000000e+00],\n",
       "       [ 1.62728000e+05,  1.75455368e+00, -6.99398285e-01, ...,\n",
       "        -5.85290816e-03, -1.96220758e-02,  9.48000000e+01],\n",
       "       [ 7.85760000e+04, -7.07635006e-01,  4.93302266e-01, ...,\n",
       "        -3.85588609e-01, -2.05588932e-01,  5.78000000e+01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "train_x_flatten = train_x_orig.values.reshape(train_x_orig.shape[0], -1)\n",
    "train_x_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = (30, 227845)\n",
      "number of test examples = 56962\n",
      "X_train shape: (30, 227845)\n",
      "Y_train shape: (2, 227845)\n",
      "X_test shape: (30, 56962)\n",
      "Y_test shape: (2, 56962)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "train_x_flatten = train_x_orig.values.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.values.reshape(test_x_orig.shape[0], -1).T\n",
    "train_y_flatten = train_y.values.reshape(train_y.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_y_flatten = test_y.values.reshape(test_y.shape[0], -1).T\n",
    "\n",
    "#train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "#test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "#train_y_flatten = train_y.reshape(train_y.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "#test_y_flatten = test_y.reshape(test_y.shape[0], -1).T\n",
    "\n",
    "X_train = train_x_flatten\n",
    "Y_train = train_y_flatten\n",
    "X_test = test_x_flatten\n",
    "Y_test = test_y_flatten\n",
    "\n",
    "Y_train = convert_to_one_hot(Y_train.astype(int), 2)\n",
    "Y_test = convert_to_one_hot(Y_test.astype(int), 2)\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape))\n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "\n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "\n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "\n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.compat.v1.placeholder(tf.float32, [n_x,None])\n",
    "    Y = tf.compat.v1.placeholder(tf.float32, [n_y,None])\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"Placeholder:0\", shape=(30, None), dtype=float32)\n",
      "Y = Tensor(\"Placeholder_1:0\", shape=(2, None), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(30, 2)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "\n",
    "    tf.compat.v1.set_random_seed(1)\n",
    "\n",
    "    W1 = tf.compat.v1.get_variable(\"W1\", [25,30], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed = 1))\n",
    "    b1 = tf.compat.v1.get_variable(\"b1\", [25,1], initializer = tf.compat.v1.zeros_initializer())\n",
    "    W2 = tf.compat.v1.get_variable(\"W2\", [12,25], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed = 1))\n",
    "    b2 = tf.compat.v1.get_variable(\"b2\", [12,1], initializer = tf.compat.v1.zeros_initializer())\n",
    "    W3 = tf.compat.v1.get_variable(\"W3\", [2,12], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed = 1))\n",
    "    b3 = tf.compat.v1.get_variable(\"b3\", [2,1], initializer = tf.compat.v1.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0216 16:22:37.674734 139713218598720 deprecation.py:506] From /home/anjana/tensorflow/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(25, 30) dtype=float32>\n",
      "b1 = <tf.Variable 'b1:0' shape=(25, 1) dtype=float32>\n",
      "W2 = <tf.Variable 'W2:0' shape=(12, 25) dtype=float32>\n",
      "b2 = <tf.Variable 'b2:0' shape=(12, 1) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    parameters = initialize_parameters()\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "\n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the parameters from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "\n",
    "    #                                                                   # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1)                                     # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                                 # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2)                                    # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                                 # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3,A2),b3)                                    # Z3 = np.dot(W3,Z2) + b3\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 = Tensor(\"Add_2:0\", shape=(2, None), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    X, Y = create_placeholders(30, 2)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    print(\"Z3 = \" + str(Z3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "\n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "\n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "\n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(a=Z3)\n",
    "    labels = tf.transpose(a=Y)\n",
    "    cost = tf.reduce_mean(input_tensor=tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    X, Y = create_placeholders(30, 2)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "\n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.compat.v1.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "\n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "\n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3, Y)\n",
    "\n",
    "    # Backpropagation: Define the tensorflow optimizer. Using AdamOptimizer.\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    # Initialize all the variables\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "    # Saver for model\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\",\n",
    "                # the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer,cost], feed_dict = {X:minibatch_X, Y: minibatch_Y})\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(input=Z3), tf.argmax(input=Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "\n",
    "        saver.save(sess, './my_test_model')\n",
    "\n",
    "        return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 174.062966\n",
      "Cost after epoch 100: 0.114000\n",
      "Cost after epoch 200: 0.004611\n",
      "Cost after epoch 300: 0.005087\n",
      "Cost after epoch 400: 0.004167\n",
      "Cost after epoch 500: 0.004150\n",
      "Cost after epoch 600: 0.003919\n",
      "Cost after epoch 700: 0.004421\n",
      "Cost after epoch 800: 0.004086\n",
      "Cost after epoch 900: 0.004239\n",
      "Cost after epoch 1000: 0.004527\n",
      "Cost after epoch 1100: 0.005186\n",
      "Cost after epoch 1200: 0.005092\n",
      "Cost after epoch 1300: 0.004645\n",
      "Cost after epoch 1400: 0.005285\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.9988984\n",
      "Test Accuracy: 0.99885887\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \n",
    "    W1 = tf.convert_to_tensor(value=parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(value=parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(value=parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(value=parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(value=parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(value=parameters[\"b3\"])\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    \n",
    "    #x = tf.compat.v1.placeholder(\"float\", [12288, 30])\n",
    "    x = tf.compat.v1.placeholder(\"float\", [30, 56962])\n",
    "    \n",
    "    \n",
    "    z3 = forward_propagation_for_predict(x, params)\n",
    "    p = tf.argmax(input=z3)\n",
    "    \n",
    "    sess = tf.compat.v1.Session()\n",
    "    prediction = sess.run(p, feed_dict = {x: X})\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_for_predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3'] \n",
    "                                                           # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 30)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters[\"W1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 56962)\n",
      "[0 0 0 ... 0 0 0]  total length:  56962\n",
      "[0. 0. 0. ... 0. 0. 0.]  total length:  56962\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "prediction = predict(X_test, parameters)\n",
    "print(prediction , \" total length: \" , len(prediction))\n",
    "print(Y_test[1] , \" total length: \", len(Y_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prediction, labels):\n",
    "    \"\"\" Returns the accuracy of the model. \n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    # A boolean or int array indicating correct predictions\n",
    "    correct_predictions = [(int(a) and int(b)) or (not int(a) and not int(b)) for a, b in zip(prediction, labels)]\n",
    "    \n",
    "    # The accuracy, number of correct predictions divided by all predictions\n",
    "    accuracy = sum(correct_predictions)/len(correct_predictions)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frauds in the test data  87.0\n",
      "Total frauds in the prediction  28\n",
      "Accuracy: 0.9989\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy of the model\n",
    "print(\"Total frauds in the test data \", sum(Y_test[1]))\n",
    "print(\"Total frauds in the prediction \", sum(prediction))\n",
    "accuracy = evaluate(prediction, Y_test[1])\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56872 3 62 25\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test[1], prediction).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
